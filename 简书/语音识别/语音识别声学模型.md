

#  利用thchs30为例建立一个语音识别系统

* 数据处理
* 搭建模型
  DFCNN
  论文地址：http://xueshu.baidu.com/usercenter/paper/show?paperid=be5348048dd263aced0f2bdc75a535e8&site=xueshu_se
  代码地址：https://github.com/audier/my_ch_speech_recognition/tree/master/tutorial

#### 语言模型代码实践tutorial也有啦：

基于CBHG结构:https://blog.csdn.net/chinatelecom08/article/details/85048019
基于自注意力机制:https://blog.csdn.net/chinatelecom08/article/details/85051817

##  1. 特征提取
input为输入音频数据，需要转化为频谱图数据，然后通过 cnn 处理图片的能力进行识别。

### 1. 读取音频文件

```python
import scipy.io.wavfile as wav  #scipy中专门处理wav的函数
import matplotlib.pyplot as plt
import os

# 随意搞个音频做实验
filepath = 'test.wav'  #文件路径

fs, wavsignal = wav.read(filepath)   #读取音频,并返回fs(频率),及信号大小

plt.plot(wavsignal)  #将信号画出来
plt.show()
```

执行结果:

![image.png](https://upload-images.jianshu.io/upload_images/14555448-d79cd0bc5cc0f274.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

### 2.构造汉明窗

汉明窗的公式;

> $w(n)=\left[0.54-0.46 \cos \left(\frac{2 \pi \cdot n}{N-1}\right)\right] R_{N}(n)$   $0<n<N$

为什么构造汉明窗:

> 语音信号一般在10ms到30ms之间，我们可以把它看成是平稳的。为了处理语音信号，我们要对语音信号进行加窗，也就是一次仅处理窗中的数据。因为实际的语音信号是很长的，我们不能也不必对非常长的数据进行一次性处理。明智的解决办法就是每次取一段数据，进行分析，然后再取下一段数据，再进行分析。
>
>  
>
> 怎么仅取一段数据呢？一种方式就是构造一个函数。这个函数在某一区间有非零值，而在其余区间皆为0.汉明窗就是这样的一种函数。它主要部分的形状像sin（x）在0到pi区间的形状，而其余部分都是0.这样的函数乘上其他任何一个函数f，f只有一部分有非零值。
>
>  
>
> 为什么汉明窗这样取呢？因为之后我们会对汉明窗中的数据进行FFT，它假设一个窗内的信号是代表一个周期的信号。（也就是说窗的左端和右端应该大致能连在一起）而通常一小段音频数据没有明显的周期性，加上汉明窗后，数据形状就有点周期的感觉了。
>
>  
>
> 因为加上汉明窗，只有中间的数据体现出来了，两边的数据信息丢失了，所以等会移窗的时候，只会移1/3或1/2窗，这样被前一帧或二帧丢失的数据又重新得到了体现。
>
>  
>
> 简单的说汉明窗就是个函数，它的形状像窗，所以类似的函数都叫做窗函数。希望你能明白。

```python
#这里就是400个点使用的汉明窗以后使用不用再改动
import numpy as np
import matplotlib.pyplot as plt

x=np.linspace(0, 400 - 1, 400, dtype = np.int64)  #画出400个从0-399的点

w = 0.54 - 0.46 * np.cos(2 * np.pi * (x) / (400 - 1))  #汉明窗的构造
plt.plot(w)
plt.show()
```

执行结果:

![image.png](https://upload-images.jianshu.io/upload_images/14555448-7e9df9e0e3459409.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

###  对数据分帧

- 帧长： 25ms
- 帧移： 10ms

```bash
1 采样点（s） = fs       #采样频率是每秒fs这里是16KHz ,也就是说每秒采样的个数就是16000个
2 采样点（ms）= fs / 1000  #每毫秒采样的个数就是fs/1000 
3 采样点（帧）= fs / 1000 * 帧长   #要求的帧长是25ms因此要求25ms采样点的个数,就是fs/1000 *帧长

```

```bash
1 time_window = 25  
2 window_length = fs // 1000 * time_window   #这里取整了,应该是采样个数为整数的原因
```

###   4. 分帧加窗

```python
# 分帧
p_begin = 0
p_end = p_begin + window_length   #帧长
frame = wavsignal[p_begin:p_end]   #取出信号(400个采样点)
plt.plot(frame)
plt.show()  

# 加窗
frame = frame * w  #利用汉明窗对取出的采样点 进行加窗
plt.plot(frame)
plt.show()
```

![分帧](https://upload-images.jianshu.io/upload_images/14555448-83f5ec7496aeaaa8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![加窗](https://upload-images.jianshu.io/upload_images/14555448-5f1f24b5555f8117.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

###  5.傅里叶变换

所谓时频图就是将时域信息转换到频域上去，具体原理可百度。人耳感知声音是通过

```python
from scipy.fftpack import fft

# 进行快速傅里叶变换
frame_fft = np.abs(fft(frame))[:200]  #因为是对称的取一半就可以了[:200],左右对称
plt.plot(frame_fft)
plt.show()

# 取对数，求db
frame_log = np.log(frame_fft)  #取对数
plt.plot(frame_log)
plt.show()
```

执行结果:

![image.png](https://upload-images.jianshu.io/upload_images/14555448-c7dab785018ff834.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![image.png](https://upload-images.jianshu.io/upload_images/14555448-04b36e68559736ab.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

###  6汇总

- 分帧
- 加窗
- 傅里叶变换

```python
import numpy as np
import scipy.io.wavfile as wav
from scipy.fftpack import fft


# 获取信号的时频图
def compute_fbank(file):
	x=np.linspace(0, 400 - 1, 400, dtype = np.int64)  #生成一个0-399的x坐标
	w = 0.54 - 0.46 * np.cos(2 * np.pi * (x) / (400 - 1) ) # 汉明窗公式
	fs, wavsignal = wav.read(file)  #返回采样频率及信号
	# wav波形 加时间窗以及时移10ms
	time_window = 25 # 单位ms每一帧是25ms
	window_length = fs / 1000 * time_window # 计算窗长度的公式，目前全部为400固定值
	wav_arr = np.array(wavsignal)
	wav_length = len(wavsignal)   #输入的语音的长度
	range0_end = int(wav_length/fs*1000 - time_window) // 10 # 计算循环终止的位置，也就是最终生成的窗数
	data_input = np.zeros((range0_end, 200), dtype = np.float) # 用于存放最终的频率特征数据
	data_line = np.zeros((1, 400), dtype = np.float)
	for i in range(0, range0_end):
		p_start = i * 160
		p_end = p_start + int(window_length)   #window_length目前就是固定的400,这里必须是整型
		data_line = wav_arr[p_start:p_end]	
		data_line = data_line * w # 加窗
		data_line = np.abs(fft(data_line))
		data_input[i]=data_line[0:200] # 设置为400除以2的值（即200）是取一半数据，因为是对称的
	data_input = np.log(data_input + 1)
	#data_input = data_input[::]
	return data_input



```

- 该函数提取音频文件的时频图

```python
import matplotlib.pyplot as plt
filepath = 'test.wav'   #文件路径

a = compute_fbank(filepath)  #获取信号的时频图
#取转置的原因我们需要将横坐标是时间,纵坐标是每一帧对应得取了傅里叶变换及对数的值
#origin = 'lower'的作用就是让纵坐标从低向上,如果不加默认的是从高往低(200---0)
plt.imshow(a.T, origin = 'lower')  #以时间为横坐标,就是25ms一帧对应一个横坐标.这一帧对应的200个点为纵坐标
plt.show()
```

执行结果:

![image.png](https://upload-images.jianshu.io/upload_images/14555448-6cb67791ac1ca101.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)



## 2. 数据处理

#### 下载数据

thchs30: <http://www.openslr.org/18/>

### 2.1 生成音频文件和标签文件列表
考虑神经网络训练过程中接收的输入输出。首先需要batch_size内数据需要统一数据的shape。

#### 格式为：[batch_size, time_step, feature_dim] (每次训练使用的数据量,步长,特征维度)

然而读取的每一个sample的时间轴长都不一样，所以需要对时间轴进行处理，选择batch内最长的那个时间为基准，进行padding。这样一个batch内的数据都相同，就能进行并行训练啦。

```python
source_file = 'data_thchs30'
```

#### 定义函数`source_get`，获取音频文件及标注文件列表

形如：

```python
['F:\\语音识别\\语音数据集\\data_thchs30/data\\A11_0.wav.trn', 
 'F:\\语音识别\\语音数据集\\data_thchs30/data\\A11_1.wav.trn',
 'F:\\语音识别\\语音数据集\\data_thchs30/data\\A11_10.wav.trn', 
 'F:\\语音识别\\语音数据集\\data_thchs30/data\\A11_100.wav.trn', 
 'F:\\语音识别\\语音数据集\\data_thchs30/data\\A11_101.wav.trn',
 'F:\\语音识别\\语音数据集\\data_thchs30/data\\A11_102.wav.trn',
 'F:\\语音识别\\语音数据集\\data_thchs30/data\\A11_103.wav.trn',
 'F:\\语音识别\\语音数据集\\data_thchs30/data\\A11_104.wav.trn', 
 'F:\\语音识别\\语音数据集\\data_thchs30/data\\A11_105.wav.trn',
 'F:\\语音识别\\语音数据集\\data_thchs30/data\\A11_106.wav.trn']

```

```python
['F:\\语音识别\\语音数据集\\data_thchs30/data\\A11_0.wav',
'F:\\语音识别\\语音数据集\\data_thchs30/data\\A11_1.wav', 
'F:\\语音识别\\语音数据集\\data_thchs30/data\\A11_10.wav',
'F:\\语音识别\\语音数据集\\data_thchs30/data\\A11_100.wav', 
'F:\\语音识别\\语音数据集\\data_thchs30/data\\A11_101.wav', 
'F:\\语音识别\\语音数据集\\data_thchs30/data\\A11_102.wav', 
'F:\\语音识别\\语音数据集\\data_thchs30/data\\A11_103.wav',
'F:\\语音识别\\语音数据集\\data_thchs30/data\\A11_104.wav',
'F:\\语音识别\\语音数据集\\data_thchs30/data\\A11_105.wav',
'F:\\语音识别\\语音数据集\\data_thchs30/data\\A11_106.wav']
```



函数:

```python
import os

def source_get(source_file):  #需要获取的文件路径
    train_file = source_file + '/data'   #这个路径下的data文件夹保存的是音频问价及标注文件
    
    label_lst = []  #存储的是标准文件
    wav_lst = []   #存储的音频文件名
    for root, dirs, files in os.walk(train_file):
        for file in files:
            if file.endswith('.wav') or file.endswith('.WAV'):
                wav_file = os.sep.join([root, file])
                label_file = wav_file + '.trn'    #这个样子比较个毛线啊?检查for循环出不出错?智障吗?应该写两个判断条件,分别的检测后缀文件
                wav_lst.append(wav_file)
                label_lst.append(label_file)
            
    return label_lst, wav_lst

label_lst, wav_lst = source_get('F:\\语音识别\\语音数据集\\data_thchs30')

print(label_lst[:10])  #标注文件
print(wav_lst[:10])   #音频文件的路径及其名称
```

> 标注文件的格式,汉字,拼音,因素
>
>  绿 是 阳春 烟 景 大块 文章 的 底色 四月 的 林 峦 更是 绿 得 鲜活 秀媚 诗意 盎然
> lv4 shi4 yang2 chun1 yan1 jing3 da4 kuai4 wen2 zhang1 de5 di3 se4 si4 yue4 de5 lin2 luan2 geng4 shi4 lv4 de5 xian1 huo2 xiu4 mei4 shi1 yi4 ang4 ran2
> l v4 sh ix4 ii iang2 ch un1 ii ian1 j ing3 d a4 k uai4 uu un2 zh ang1 d e5 d i3 s e4 s iy4 vv ve4 d e5 l in2 l uan2 g eng4 sh ix4 l v4 d e5 x ian1 h uo2 x iu4 m ei4 sh ix1 ii i4 aa ang4 r an2



#### 确认相同id对应的音频文件和标签文件相同



```python
for i in range(10000):
    wavname = (wav_lst[i].split('/')[-1]).split('.')[0]   #获取的就是前面的前缀名称,但是没有什么太大的意义
    labelname = (label_lst[i].split('/')[-1]).split('.')[0]
    if wavname != labelname:
        print('error')
```

### 2.2 label数据处理

#### 定义函数`read_label`读取音频文件对应的拼音label

```python
#这里要搭配上面定义的函数一起使用
#定义读取该文件的文字
def read_label(label_file):
    with open(label_file, 'r', encoding='utf8') as f:
        data = f.readlines()  #readlines()方法读取整个文件所有行，保存在一个列表(list)变量中，每行作为一个元素，但读取大文件会比较占内存
        return data[1]   #这里返回的是拼音这一行(第二行)

print(read_label(label_lst[0]))  #这里打印第一个文件测试一下

def gen_label_data(label_lst):
    label_data = []
    for label_file in label_lst:
        pny = read_label(label_file)
        label_data.append(pny.strip('\n'))
    return label_data   #这里返回的是一个list存储了所有的拼音串

label_data = gen_label_data(label_lst)
print(len(label_data))
```

执行结果预览:label_data存储的是13388行拼音串

```python
lv4 shi4 yang2 chun1 yan1 jing3 da4 kuai4 wen2 zhang1 de5 di3 se4 si4 yue4 de5 lin2 luan2 geng4 shi4 lv4 de5 xian1 huo2 xiu4 mei4 shi1 yi4 ang4 ran2

13388
```

#### 为label建立拼音到id的映射，即词典(vocab)

```python
#为label建立拼音到id的映射，即词典
def mk_vocab(label_data):  #传入的就是前面做好的包含所有拼音串的列表
    vocab = []  #新建一个空的list
    for line in label_data:  #从拼音串列表中每行取出
        line = line.split(' ')  #以空格进行分割,存储成一个多行的list每个拼音是一行
        for pny in line:  #对这一个单独的拼音list进行遍历
            if pny not in vocab:  #如果这个单独的拼音不在总的拼音字符中就进行添加,这样就可以去除重复,保留的都是不相同的拼音
                vocab.append(pny)
    vocab.append('_')   #最后添加一个_作为终止符
    return vocab

vocab = mk_vocab(label_data)
print(len(vocab))
```

执行结果:可以看出总共有1209个拼音

```python
1209
```



#### 有了词典就能将读取到的label映射到对应的id

```python
#有了词典就能将读取到的label映射到对应的id
def word2id(line, vocab):  #输入的是需要转换成id的line,以及词典
    return [vocab.index(pny) for pny in line.split(' ')] #使用的是列表生成器


label_id = word2id(label_data[0], vocab)  #这里测试的数据的第一行
print(label_data[0])
print(label_id)
```



执行结果:因为是第一行,因此编号连续,其他行就不一样了

```python
lv4 shi4 yang2 chun1 yan1 jing3 da4 kuai4 wen2 zhang1 de5 di3 se4 si4 yue4 de5 lin2 luan2 geng4 shi4 lv4 de5 xian1 huo2 xiu4 mei4 shi1 yi4 ang4 ran2
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 10, 15, 16, 17, 1, 0, 10, 18, 19, 20, 21, 22, 23, 24, 25]
```

#### 总结:

我们提取出了每个音频文件对应的拼音标签`label_data`，通过索引就可以获得该索引的标签。

也生成了对应的拼音词典.由此词典，我们可以映射拼音标签为id序列。

输出：

- vocab
- label_data

完整的程序:

```python
# -*- coding: utf-8 -*-
"""
Created on Mon Jun  3 09:54:10 2019

@author: zangz
"""
import os

def source_get(source_file):  #需要获取的文件路径
    train_file = source_file + '/data'   #这个路径下的data文件夹保存的是音频问价及标注文件
    
    label_lst = []  #存储的是标准文件
    wav_lst = []   #存储的音频文件名
    for root, dirs, files in os.walk(train_file):
        for file in files:
            if file.endswith('.wav') or file.endswith('.WAV'):
                wav_file = os.sep.join([root, file])
                label_file = wav_file + '.trn'  
                wav_lst.append(wav_file)
                label_lst.append(label_file)
            
    return label_lst, wav_lst

label_lst, wav_lst = source_get('F:\\语音识别\\语音数据集\\data_thchs30')

print(label_lst[:10])  #标注文件
print(wav_lst[:10])   #音频文件的路径及其名称


for i in range(10000):
    wavname = (wav_lst[i].split('/')[-1]).split('.')[0]   #获取的就是前面的前缀名称,但是没有什么太大的意义
    labelname = (label_lst[i].split('/')[-1]).split('.')[0]
    if wavname != labelname:
        print('error')

#这里要搭配上面定义的函数一起使用
#定义读取该文件的文字
def read_label(label_file):
    with open(label_file, 'r', encoding='utf8') as f:
        data = f.readlines()  #readlines()方法读取整个文件所有行，保存在一个列表(list)变量中，每行作为一个元素，但读取大文件会比较占内存
        return data[1]   #这里返回的是拼音这一行(第二行)

print(read_label(label_lst[0]))  #这里打印第一个文件测试一下

def gen_label_data(label_lst):
    label_data = []
    for label_file in label_lst:
        pny = read_label(label_file)
        label_data.append(pny.strip('\n'))
    return label_data   #这里返回的是一个list存储了所有的拼音串

label_data = gen_label_data(label_lst)
print(len(label_data))




#为label建立拼音到id的映射，即词典
def mk_vocab(label_data):  #传入的就是前面做好的包含所有拼音串的列表
    vocab = []  #新建一个空的list
    for line in label_data:  #从拼音串列表中每行取出
        line = line.split(' ')  #以空格进行分割,存储成一个多行的list每个拼音是一行
        for pny in line:  #对这一个单独的拼音list进行遍历
            if pny not in vocab:  #如果这个单独的拼音不在总的拼音字符中就进行添加,这样就可以去除重复,保留的都是不相同的拼音
                vocab.append(pny)
    vocab.append('_')   #最后添加一个_作为终止符
    return vocab

vocab = mk_vocab(label_data)
print(len(vocab))

#有了词典就能将读取到的label映射到对应的id
def word2id(line, vocab):  #输入的是需要转换成id的line,以及词典
    return [vocab.index(pny) for pny in line.split(' ')] #使用的是列表生成器


label_id = word2id(label_data[0], vocab)  #这里测试的数据的第一行
print(label_data[0])
print(label_id)

```

程序测试:

```python
print(vocab[:15])  #词典的前15行
print(label_data[10]) #数据的第11个
print(word2id(label_data[10], vocab)) #这个数据的映射ID
```

执行结果:

```python
['lv4', 'shi4', 'yang2', 'chun1', 'yan1', 'jing3', 'da4', 'kuai4', 'wen2', 'zhang1', 'de5', 'di3', 'se4', 'si4', 'yue4']
xiang1 gang3 yan3 yi4 quan1 huan1 ying2 mao2 a1 min3 jia1 meng2 wu2 xian4 tai2 yu3 hua2 xing1 yi4 xie1 zhong4 da4 de5 yan3 chang4 huo2 dong4 dou1 yao1 qing3 ta1 chu1 chang3 you2 ji3 ci4 hai2 te4 yi4 an1 pai2 ya1 zhou4 yan3 chu1
[143, 225, 53, 23, 226, 227, 228, 229, 230, 231, 148, 232, 233, 90, 234, 94, 235, 236, 23, 237, 238, 6, 10, 53, 239, 19, 40, 203, 29, 240, 26, 108, 241, 242, 198, 194, 157, 181, 23, 161, 243, 244, 245, 53, 108]
```





### 2.3 音频数据处理

音频数据处理，只需要获得对应的音频文件名，然后提取所需时频图即可。

其中`compute_fbank`时频转化的函数在前面已经定义好了。(使用之前导包就好了,将这个函数导入到这个文件中)

```python
fbank = compute_fbank(wav_lst[0])  #将第一个音频文件转换成视频图

print(fbank.shape)  #打印时频矩阵的形状
```

执行 结果:这里就是将这个音频以25ms一帧时移为10ms切成了777帧,每一帧的特征矩阵 都是200维

```python
(777, 200)
```



绘制时频图:

```python

plt.imshow(fbank.T, origin = 'lower')
plt.show()
```

![image.png](https://upload-images.jianshu.io/upload_images/14555448-91098c87abd64178.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

#### 由于声学模型网络结构原因（3个maxpooling层），我们的音频数据的每个维度需要能够被8整除。(将不能整除的部分舍弃掉)

```python
fbank = fbank[:fbank.shape[0]//8*8, :]
print(fbank.shape)
```

执行结果:

```python
(776, 200)
```

#### 总结：

- 对音频数据进行时频转换
- 转换后的数据需要各个维度能够被8整除

### 2.4 数据生成器

#### 确定batch_size和batch_num  

```python
total_nums = 10000   #总的数据量为10000
batch_size = 4       #每次拿4个数据做训练
batch_num = total_nums // batch_size  #总共多少次才能训练完一个周期
```

#### shuffle

打乱数据的顺序，我们通过查询乱序的索引值，来确定训练数据的顺序(这样就不是顺序的读取造成每次都是取一个人的数据)

```python
from random import shuffle
shuffle_list = [i for i in range(10000)]   #先生称0-10000的顺序列表
shuffle(shuffle_list)  #进行打乱
```

执行的结果:(乱序的列表大概就是这样的)

![image.png](https://upload-images.jianshu.io/upload_images/14555448-5c580996d9db66a4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)



#### generator

batch_size(每次训练需要的数据量)的信号时频图和标签数据，存放到两个list中去

```python
#数据生成器,需要传入的每次训练数据的大小,乱序的列表,音频文件路径,标注文件路径,建立好的词典映射
def get_batch(batch_size, shuffle_list, wav_lst, label_data, vocab):
    for i in range(10000//batch_size):  #10000//batch_size 一个周期循环取这么多次
        wav_data_lst = []  #每次循环建立一个空的存放音频文件路径的列表
        label_data_lst = []   #每次循环建立一个空的存放标注文件路径的列表
        begin = i * batch_size  #每次取数据的起始索引
        end = begin + batch_size #每次取数据的结束索引
        sub_list = shuffle_list[begin:end]  #取出乱序的索引
        for index in sub_list:  #按照乱序的索引取数据
            fbank = compute_fbank(wav_lst[index]) #取出音频生成时频矩阵
            fbank = fbank[:fbank.shape[0] // 8 * 8, :] #对时频矩阵进行处理保证能被8整除
            label = word2id(label_data[index], vocab) #取出标注数据转换成字典映射
            wav_data_lst.append(fbank)  
            label_data_lst.append(label)  #将处理好的时频矩阵及字典映射好的标签进行存储
        yield wav_data_lst, label_data_lst    #使用yield生成器进行存储,类似于链表

batch = get_batch(4, shuffle_list, wav_lst, label_data, vocab)   
```

```python
wav_data_lst, label_data_lst = next(batch)  #每次取数据使用的是next函数(对应的是yield生成器),每次执行都会取出下一个对应的数据类似链表的作用
#循环打印一下取出的4个数据组
for wav_data in wav_data_lst:
    print(wav_data.shape)
for label_data in label_data_lst:
    print(label_data)

```

每次执行上面的next返回的都是下一个节点对应的数据

第一次:

```python
(992, 200)
(1016, 200)
(1120, 200)
(976, 200)
[26, 303, 99, 342, 102, 10, 35, 110, 519, 744, 9, 167, 272, 745, 295, 178, 744, 369, 126, 282, 438, 746, 146, 315, 371, 272, 472, 245, 388, 286, 721, 659, 1, 282, 101, 272, 410, 497, 560, 42]
[558, 66, 238, 597, 773, 717, 56, 517, 280, 553, 421, 94, 140, 729, 127, 228, 684, 674, 729, 393, 94, 286, 444, 729, 393, 417, 376, 10, 143, 411, 80, 367]
[175, 874, 260, 668, 283, 626, 398, 11, 136, 692, 527, 643, 892, 56, 364, 160, 14, 166, 295, 963, 30, 942, 238, 175, 874, 260, 961, 65, 43, 213, 172, 376]
[516, 254, 532, 432, 31, 371, 823, 35, 168, 580, 580, 580, 103, 78, 826, 562, 168, 377, 348, 604, 243, 384, 168, 102, 257, 682, 168, 377, 348, 673, 243, 384, 101, 344, 13, 102, 257, 682]
```

第二次:

```python
(744, 200)
(1064, 200)
(1024, 200)
(976, 200)
[26, 351, 624, 178, 198, 30, 27, 233, 69, 330, 465, 214, 84, 242, 625, 94, 99, 413, 626, 627, 131, 628, 14, 14, 286, 1, 10, 171, 84]
[303, 62, 304, 273, 305, 203, 30, 306, 307, 308, 282, 30, 196, 158, 309, 310, 146, 155, 311, 312, 313, 146, 314, 315, 10, 316, 317, 106, 307, 275, 30, 306, 318]
[33, 371, 365, 738, 450, 166, 26, 293, 676, 787, 254, 196, 228, 71, 881, 1, 241, 183, 234, 486, 659, 75, 69, 761, 718, 42, 228, 71, 881, 376, 380, 90, 69, 761, 254, 196, 228, 71, 881, 376, 380, 90]
[153, 81, 699, 305, 80, 82, 23, 242, 190, 75, 33, 689, 107, 689, 115, 216, 172, 649, 252, 322, 410, 148, 337, 80, 82, 91, 1159, 36, 56, 472, 305]
```

四个时频矩阵的形状大小:

```python
lens = [len(wav) for wav in wav_data_lst]  #取出的四个数据中时频矩阵的行的大小
print(max(lens))     #查找最大的一个
print(lens)
```

执行结果:

```python
1064
[744, 1064, 1024, 976]
```

#### padding

然而，每一个batch_size内的数据有一个要求，就是需要构成一个tensorflow块，这就要求每个样本数据**形式**是一样的。
除此之外，ctc需要获得的信息还有输入序列的长度。
这里输入序列经过卷积网络后，长度缩短了8倍，因此我们训练实际输入的数据为wav_len//8(传递给CTC的)。

* padding wav data

* wav len // 8 （网络结构导致的）

```python
def wav_padding(wav_data_lst):
    wav_lens = [len(data) for data in wav_data_lst]
    wav_max_len = max(wav_lens)
    wav_lens = np.array([leng//8 for leng in wav_lens])  #将四个时频矩阵的行长度除以8再进行存储,因为卷积后长度缩小8倍输入给CTC需要给定长度,所以需要将长度提前存成缩小为8倍的
    new_wav_data_lst = np.zeros((len(wav_data_lst), wav_max_len, 200, 1)) #这里需要生成的传递给tensorflow的数组,第一位是每次训练的数据量,第二维是,四个数据时频矩阵的最大长度(需要统一长度,不够的默认补零),第三维就是时频矩阵的列维度(200),第四维是通道数(这里是1)
    for i in range(len(wav_data_lst)): #循环存储
        new_wav_data_lst[i, :wav_data_lst[i].shape[0], :, 0] = wav_data_lst[i]#赋值给第二列
    return new_wav_data_lst, wav_lens  #返回处理好的一训练需要的传递给tensorflow的数据及音频除以8的音频长度

pad_wav_data_lst, wav_lens = wav_padding(wav_data_lst)
print(pad_wav_data_lst.shape)
print(wav_lens)
```

执行结果:  后面的是除以8的音频长度,可以看出1064=133*8

```python
(4, 1064, 200, 1)
[ 93 133 128 122]
```



同样也要对label进行padding和长度获取，不同的是数据维度不同，且label的长度就是输入给ctc的长度，**不需要额外处理**

- label padding
- label len

```python
def label_padding(label_data_lst):  #传入的是标注矩阵
    label_lens = np.array([len(label) for label in label_data_lst]) #获取每个label矩阵的长度
    max_label_len = max(label_lens) #最大长度
    new_label_data_lst = np.zeros((len(label_data_lst), max_label_len))  #生成一个4行的label矩阵(其他的不够长度的补零跟前面一致)
    for i in range(len(label_data_lst)):
        new_label_data_lst[i][:len(label_data_lst[i])] = label_data_lst[i]
    return new_label_data_lst, label_lens

pad_label_data_lst, label_lens = label_padding(label_data_lst)
print(pad_label_data_lst.shape)
print(label_lens)
#print(pad_label_data_lst)
```

执行结果:

```python
(4, 42)
[29 33 42 31]


```

长度不够的进行了补零的操作

![长度不够的进行了补零操作](https://upload-images.jianshu.io/upload_images/14555448-288421ca02e20235.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

#### 用于训练格式的数据生成器(对前面函数的一个汇总)(后面的输入输出后面看过在完整的分析)

```python
def data_generator(batch_size, shuffle_list, wav_lst, label_data, vocab):
    for i in range(len(wav_lst)//batch_size):
        wav_data_lst = []
        label_data_lst = []
        begin = i * batch_size
        end = begin + batch_size
        sub_list = shuffle_list[begin:end]
        for index in sub_list:
            fbank = compute_fbank(wav_lst[index])
            pad_fbank = np.zeros((fbank.shape[0]//8*8+8, fbank.shape[1]))
            pad_fbank[:fbank.shape[0], :] = fbank
            label = word2id(label_data[index], vocab)
            wav_data_lst.append(pad_fbank)
            label_data_lst.append(label)
        pad_wav_data, input_length = wav_padding(wav_data_lst)
        pad_label_data, label_length = label_padding(label_data_lst)
        inputs = {'the_inputs': pad_wav_data,
                  'the_labels': pad_label_data,
                  'input_length': input_length,
                  'label_length': label_length,
                 }
        outputs = {'ctc': np.zeros(pad_wav_data.shape[0],)} 
        yield inputs, outputs

```

