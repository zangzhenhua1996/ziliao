深度神经网络**（Deep Neural Networks， 以下简称DNN）**是深度学习的基础，而要理解DNN，首先我们要理解DNN模型，下面我们就对DNN的模型与前向传播算法做一个总结。
## 1. 从感知机到神经网络
在[感知机原理小结](http://www.cnblogs.com/pinard/p/6042320.html)中，我们介绍过感知机的模型，它是一个有若干输入和一个输出的模型，如下图:
![image.png](https://upload-images.jianshu.io/upload_images/14555448-15e644673bdcee00.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
输出和输入之间学习到一个线性关系，得到中间输出结果：$$z=\sum_{i=1}^{m} w_{i} x_{i}+b$$
　接着是一个神经元激活函数:$$
\operatorname{sign}(z)=\left\{\begin{array}{ll}{-1} & {z<0} \\ {1} & {z \geq 0}\end{array}\right.
$$
从而得到我们想要的输出结果$1$或者$-1$。

　　　　这个模型只能用于二元分类，且无法学习比较复杂的非线性模型，因此在工业界无法使用。

　　　　而神经网络则在感知机的模型上做了扩展，总结下主要有三点：
* 1）加入了隐藏层，隐藏层可以有多层，增强模型的表达能力，如下图实例，当然增加了这么多隐藏层模型的复杂度也增加了好多。
![image.png](https://upload-images.jianshu.io/upload_images/14555448-681a5ad59346bbde.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
* 2）输出层的神经元也可以不止一个输出，可以有多个输出，这样模型可以灵活的应用于分类回归，以及其他的机器学习领域比如降维和聚类等。多个神经元输出的输出层对应的一个实例如下图，输出层现在有4个神经元了。
![image.png](https://upload-images.jianshu.io/upload_images/14555448-034bcbc69d3306c7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
　　3） 对激活函数做扩展，感知机的激活函数是$sign(z)$,虽然简单但是处理能力有限，因此神经网络中一般使用的其他的激活函数，比如我们在逻辑回归里面使用过的$Sigmoid$函数，即：$$
f(z)=\frac{1}{1+e^{-z}}
$$
　还有后来出现的$tanx, softmax,和ReLU$等。通过使用不同的激活函数，神经网络的表达能力进一步增强。对于各种常用的激活函数，我们在后面再专门讲。
## 2. DNN的基本结构
　　　　上一节我们了解了神经网络基于感知机的扩展，而 DNN 可以理解为有很多隐藏层的神经网络。这个很多其实也没有什么度量标准, 多层神经网络和深度神经网络 DNN 其实也是指的一个东西，当然，DNN 有时也叫做多层感知机（**Multi-Layer perceptron,MLP**）, 名字实在是多。后面我们讲到的神经网络都默认为**DNN**。
　从**DNN**按不同层的位置划分，**DNN**内部的神经网络层可以分为三类，输入层，隐藏层和输出层,如下图示例，一般来说第一层是输入层，最后一层是输出层，而中间的层数都是隐藏层。
![image.png](https://upload-images.jianshu.io/upload_images/14555448-cc384232cfbbb2d3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

层与层之间是全连接的，也就是说，第$i$层的任意一个神经元一定与第$i+1$层的任意一个神经元相连。虽然**DNN**看起来很复杂，但是从小的局部模型来说，还是和感知机一样，即一个线性关系$z=\sum w_{i} x_{i}+b$加上一个激活函数$σ(z)$。
由于**DNN**层数多，则我们的线性关系系数**w**和偏置**b**的数量也就是很多了。具体的参数在**DNN**是如何定义的呢？

首先我们来看看线性关系系数$w$的定义。以下图一个三层的$DNN$为例，第二层的第$4$个神经元到第三层的第$2$个神经元的线性系数定义为$w_{24}^{3}$。上标$3$代表线性系数$w$所在的层数，而下标对应的是输出的第三层索引$2$和输入的第二层索引$4$。你也许会问，为什么不是$w_{42}^{3}$, 而是$w_{24}^{3}$呢？这主要是为了便于模型用于矩阵表示运算，如果是$w_{42}^{3}$而每次进行矩阵运算是$w^Tx+b$，需要进行转置。将输出的索引放在前面的话，则线性运算不用转置,即直接为$wx+b$。总结下，第$l−1$层的第$k$个神经元到第$l$层的第$j$个神经元的线性系数定义为$w^ljk$。注意，输入层是没有$w$参数的。
![image.png](https://upload-images.jianshu.io/upload_images/14555448-7671419cfc19b36e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

再来看看偏置$b$的定义。还是以这个三层的**DNN**为例，第二层的第三个神经元对应的偏置定义为$b^2_3$。其中，上标$2$代表所在的层数，下标$3$代表偏倚所在的神经元的索引。同样的道理，第三个的第一个神经元的偏倚应该表示为$b^3_1$。同样的，输入层是没有偏倚参数b的。(每个神经元对应的偏置值是相同的)
![image.png](https://upload-images.jianshu.io/upload_images/14555448-3a02aac362ac4d7b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


## 3. DNN前向传播算法数学原理
　　　　在上一节，我们已经介绍了$DNN$各层线性关系系数$w$,偏置$b$的定义。假设我们选择的激活函数是$σ(z)$(Sigmoid)，隐藏层和输出层的输出值为$a$，则对于下图的三层$DNN$,利用和感知机一样的思路，我们可以利用上一层的输出计算下一层的输出，也就是所谓的DNN前向传播算法。
![image.png](https://upload-images.jianshu.io/upload_images/14555448-d6169ea981488642.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

对于第二层的的输出$a_{1}^{2}, a_{2}^{2}, a_{3}^{2}$,我们有：$$
\begin{array}{l}{a_{1}^{2}=\sigma\left(z_{1}^{2}\right)=\sigma\left(w_{11}^{2} x_{1}+w_{12}^{2} x_{2}+w_{13}^{2} x_{3}+b_{1}^{2}\right)} \\ {a_{2}^{2}=\sigma\left(z_{2}^{2}\right)=\sigma\left(w_{21}^{2} x_{1}+w_{22}^{2} x_{2}+w_{23}^{2} x_{3}+b_{2}^{2}\right)} \\ {a_{3}^{2}=\sigma\left(z_{3}^{2}\right)=\sigma\left(w_{31}^{2} x_{1}+w_{32}^{2} x_{2}+w_{33}^{2} x_{3}+b_{3}^{2}\right)}\end{array}
$$

对于第三层的的输出$a^3_1$，我们有：$$
a_{1}^{3}=\sigma\left(z_{1}^{3}\right)=\sigma\left(w_{11}^{3} a_{1}^{2}+w_{12}^{3} a_{2}^{2}+w_{13}^{3} a_{3}^{2}+b_{1}^{3}\right)
$$

　将上面的例子一般化，假设第$l−1$层共有$m$个神经元，则对于第$l$层的第$j$个神经元的输出$a^l_j$，我们有：
$$
a_{j}^{l}=\sigma\left(z_{j}^{l}\right)=\sigma\left(\sum_{k=1}^{m} w_{j k}^{l} a_{k}^{l-1}+b_{j}^{l}\right)
$$
　其中，如果$l=2$,则对于的$a^1_k$即为输入层的$x_k$。

从上面可以看出，使用代数法一个个的表示输出比较复杂，而如果使用矩阵法则比较的简洁。假设第$l−1$层共有$m$个神经元，而第$l$层共有$n$个神经元，则第$l$层的线性系数$w$组成了一个$n×m$的矩阵$W^l$, 第$l$层的偏置$b$组成了一个$n×1$的向量$b^l$ , 第$l−1$层的的输出$a$组成了一个$m×1$的向量$a^{l−1}$，第$l$层的的未激活前线性输出$z$组成了一个$n×1$的向量$z^l$, 第$l$层的的输出$a$组成了一个$n×1$的向量$a^l$。则用矩阵法表示，第$l$层的输出为：$$
a^{l}=\sigma\left(z^{l}\right)=\sigma\left(W^{l} a^{l-1}+b^{l}\right)
$$
　　这个表示方法简洁漂亮，后面我们的讨论都会基于上面的这个矩阵法表示来。
## 4. DNN前向传播算法
有了上一节的数学推导，$DNN$的前向传播算法也就不难了。所谓的$DNN$的前向传播算法也就是利用我们的若干个权重系数矩阵$W$,偏置向量$b$来和输入值向量$x$进行一系列线性运算和激活运算，从输入层开始，一层层的向后计算，一直到运算到输出层，得到输出结果为值:

　输入: 总层数$L$，所有隐藏层和输出层对应的矩阵$W$,偏置向量$b$，输入值向量$x$
    输出：输出层的输出$a^L$
* 1） 初始化$a^1=x$
* 　2)  $for$ $   l=2$ $ to$ $ L$, 计算：
$$
a^{l}=\sigma\left(z^{l}\right)=\sigma\left(W^{l} a^{l-1}+b^{l}\right)
$$
## 5. DNN前向传播算法小结
　　　　单独看$DNN$前向传播算法，似乎没有什么大用处，而且这一大堆的矩阵$W$,偏置向量$b$对应的参数怎么获得呢？怎么得到最优的矩阵$W$,偏置向量$b$呢？这个我们在讲$DNN$的反向传播算法时再讲。而理解反向传播算法的前提就是理解$DNN$的模型与前向传播算法。这也是我们这一篇先讲的原因。


[转载](https://www.cnblogs.com/pinard/p/6418668.html)
