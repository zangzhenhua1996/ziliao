## 1. 均方差损失函数+Sigmoid激活函数的问题
在讲反向传播算法时，我们用均方差损失函数和Sigmoid激活函数做了实例，首先我们就来看看`均方差+Sigmoid`的组合有什么问题。

　　　　首先我们回顾下`Sigmoid`激活函数的表达式为：$$\sigma(z)=\frac{1}{1+e^{-z}}$$
$σ(z)$的函数图像如下：![image.png](https://upload-images.jianshu.io/upload_images/14555448-62e01fadd86eddba.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
　从图上可以看出，对于$Sigmoid$，当$z$的取值越来越大后，函数曲线变得越来越平缓，意味着此时的导数$σ′(z)$也越来越小。同样的，当$z$的取值越来越小时，也有这个问题。仅仅在$z$取值为$0$附近时，导数$σ′(z)$的取值较大。

　　　　在上篇讲的$均方差+Sigmoid$的反向传播算法中，每一层向前递推都要乘以$σ′(z)$,得到梯度变化值。$Sigmoid$的这个曲线意味着在大多数时候，我们的梯度变化值很小，导致我们的$W,b$更新到极值的速度较慢，也就是我们的算法收敛速度较慢。那么有什么什么办法可以改进呢？
## 2. 使用交叉熵损失函数+Sigmoid激活函数改进DNN算法收敛速度

上一节我们讲到$Sigmoid$的函数特性导致反向传播算法收敛速度慢的问题，那么如何改进呢？换掉$Sigmoid$？这当然是一种选择。另一种常见的选择是用`交叉熵损失函数`来代替`均方差损失函数`。

我们来看看二分类时每个样本的交叉熵损失函数的形式：
$$J(W, b, a, y)=-[y \ln a+(1-y) \ln (1-a)]$$
　这个形式其实很熟悉，在[逻辑回归原理小结](http://www.cnblogs.com/pinard/p/6029432.html)中其实我们就用到了类似的形式，只是当时我们是用最大似然估计推导出来的，而这个损失函数的学名叫交叉熵。

　　　　使用了交叉熵损失函数，就能解决$Sigmoid$函数导数变化大多数时候反向传播算法慢的问题吗？我们来看看当使用交叉熵时，我们输出层$\delta^{L}$的梯度情况。
$$\begin{aligned} \delta^{L} &=\frac{\partial J\left(W, b, a^{L}, y\right)}{\partial z^{L}} \\ &=-y \frac{1}{a^{L}}\left(a^{L}\right)\left(1-a^{L}\right)+(1-y) \frac{1}{1-a^{L}}\left(a^{L}\right)\left(1-a^{L}\right) \\ &=-y\left(1-a^{L}\right)+(1-y) a^{L} \\ &=a^{L}-y \end{aligned}$$
