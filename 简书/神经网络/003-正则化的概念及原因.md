## 正则化的概念及原因

简单来说，正则化是一种为了减小**测试误差**的行为(有时候会增加**训练误差**)。我们在构造机器学习模型时，最终目的是让模型在面对新数据的时候，可以有很好的表现。当你用比较复杂的模型比如神经网络，去拟合数据时，很容易出现过拟合现象，这会导致模型的泛化能力下降，这时候，我们就需要使用**正则化**，让训练出来的模型没有那么复杂。

## 正则化的几种常用方法

- L1 & L2范数

首先介绍一下范数的定义，假设 $x$是一个向量，它的 $L^p$范数定义:
 $||x||_p = (\sum_{i}^{}{|x_i|^p})^\frac{1}{p}$
 在目标函数后面添加一个系数的“惩罚项”是正则化的常用方式，为了防止系数过大从而让模型变得复杂。在加了正则化项之后的目标函数为:
 $\bar{J}(w, b) = J(w, b) + \frac{\lambda}{2m}\Omega(w)$
 式中， $\frac{\lambda}{2m}$ 是一个常数， $m$为样本个数， $\lambda$是一个超参数，用于控制正则化程度。

$L^1$ 正则化时，对应惩罚项为  :
 $\Omega(w)=||w||_1 = \sum_{i}^{}{|w_i|}$
 $L^2$正则化时，对应惩罚项为 :
 $\Omega(w)=||w||_2^2 = \sum_{i}^{}{w_i^2}$
 从上式可以看出，$L^1$正则化通过让原目标函数加上了**所有特征系数绝对值的和**来实现正则化，而$L^2$正则化通过让原目标函数加上了**所有特征系数的平方和**来实现正则化。

两者都是通过加上一个和项来限制参数大小，那为何 $L^1$ 正则化就更适用于**特征选择**，而$L^2$ 正则化更适用于**防止模型过拟合**呢？

让我们从梯度下降的角度入手，探究两者的区别。
 为了方便叙述，假设数据只有两个特征即 $w_1,w_2$ ，考虑$L^1$ 正则化的目标函数：
 $\bar{J} = J +\frac{\alpha \lambda}{2m} (|w_1| +|w_2|)$
 在每次更新$w_1$时：
 $w_1 :=w_1 - \alpha dw_1$
 $=w_1-\frac{\alpha \lambda}{2m}(\frac{\partial J}{\partial w_1} + sign(w_1))$
 $=w_1-\frac{\alpha \lambda}{2m}sign(w_1)-\frac{\partial J}{\partial w_1}$
 **若 $w_1$为正数，则每次更新会减去一个常数；若 $w_1$为负数，则每次更新会加上一个常数，所以很容易产生特征的系数为 0 的情况**，特征系数为 0 表示该特征不会对结果有任何影响，因此$L^1$正则化会让特征变得稀疏，起到特征选择的作用。

现考虑$L^2$正则化的目标函数：
 $\bar{J} = J +\frac{\alpha \lambda}{2m} (w_1^2 +w_2^2)$
 在每次更新 $w_1$时：
 $w_1 :=w_1 - \alpha dw_1$
 $=w_1-\frac{\alpha \lambda}{2m}(\frac{\partial J}{\partial w_1} + 2w_1)$
 $=(1-\frac{\alpha \lambda}{m})w_1-\frac{\alpha \lambda}{2m}\frac{\partial J}{\partial w_1}$
 从上式可以看出每次更新时，会对特征系数进行一个**比例的缩放**而不是像$L^1$正则化减去一个固定值，这会让系数趋向变小而不会变为 0，因此$L^2$正则化会让模型变得更简单，防止过拟合，而不会起到特征选择的作用。
 以上就是 $L^1, L^2$正则化的作用以及区别。

下面来看一个课程中的例子，当不使用正则化，发生过拟合时:
 ![overfitting](https://upload-images.jianshu.io/upload_images/14555448-40a3a45bbbf64bb7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)




 使用 $L^2$正则化，正常拟合


![在这里插入图片描述](https://upload-images.jianshu.io/upload_images/14555448-4a116ac52566e6db.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)



 在图中可以有比较直观的感受是，过拟合时，分类边界的起伏会更大，也就是在部分点斜率更大，而正常拟合时，分类边界更加平缓。这也是为什么在目标函数中加入“惩罚项”可以达到正则化的效果，“惩罚项”可以使每个参数趋向更小，在求导时斜率也会更小，等于变相的让模型更加简单了，更加简单的模型当然更加不容易过拟合。



## 训练集增强

更多的训练集是提升机器学习模型泛化能力最好的方法。试想一下如果你的训练集包含了所有的你需要测试的数据，那么该模型的泛化能力将是100%(开个玩笑)。在实际项目中，有时候获取更多的训练数据的成本会很高，这时候就需要我们自己来“创造”数据。希望在以后，GAN可以成功的应用到训练集增强领域。
 对于一些特定的场景，“创造”数据其实是很简单的，例如图像识别。下面有一张6，我们可以对他采用**小幅旋转，平移，放大，缩小甚至给图片加上波动**等方法，他的标签实际还是6，但是我们已经多了很多的训练数据。需要注意的是不应做翻转操作，因为6会翻转成9，这会变成一个错误的样本。



![6本来的样子](https://upload-images.jianshu.io/upload_images/14555448-ea3c9ef8e5399fca.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)






![旋转20度](https://upload-images.jianshu.io/upload_images/14555448-e6c507718cc8dd8f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)






![平移几个像素](https://upload-images.jianshu.io/upload_images/14555448-aa138d8daebaba08.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)






![把里面的6放大](https://upload-images.jianshu.io/upload_images/14555448-93a73c3113683d1a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)






![6翻了变成9](https://upload-images.jianshu.io/upload_images/14555448-3c4a8648bd15e82f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)




## dropout

dropout 是一种计算方便但功能强大的正则化方法，适用于最近很火的神经网络。他的基本步骤是在每一次的迭代中，随机删除一部分节点，只训练剩下的节点。每次迭代都会随机删除，每次迭代删除的节点也都不一样，相当于每次迭代训练的都是不一样的网络，通过这样的方式降低节点之间的关联性以及模型的复杂度，从而达到正则化的效果。这点上有点类似 bagging，但是远比 bagging 来的简单。

直观理解：



![dropout之前(左)与dropout之后(右)](https://upload-images.jianshu.io/upload_images/14555448-622c81465960b72e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)




说dropout简单是因为你只需要设置一个超参数 keep_prob，这个参数的意义是每层节点随机保留的比例，比如将 keep_prob 设置为 0.7，那么就会随机30%的节点消失，消失这个动作其实很简单，只是将这一层的参数矩阵与根据 keep_prob 生成的 {0, 1} 矩阵做 **逐点乘积**，当然前向传播与反向传播都需要做以上的操作。

dropout的缺点在于，需要将训练集分为不同子集输入每一次的迭代，这就需要较大的训练集，所以在训练集较小的情况下，dropout的效果可能并不好。我们上面也说过，增大数据集是最好的正则化方式，**所以在增大数据集的情况下，使用 dropout 需要使用的计算代价可能会比他带来正则化效果更高**，这需要我们在实际使用场景中做取舍。

## earlystopping

提前终止可能是最简单的正则化方式，他适用于模型的表达能力很强的时候。这种情况下，一般训练误差会随着训练次数的增多逐渐下降，而测试误差则会先下降而后再次上升。我们需要做的就是在测试误差最低的点停止训练即可。
 下面还是一个DL课程中的例子，分别展示了训练1500次与2500次时的训练误差与测试误差：


![训练1500次，测试集上的正确率是82%](https://upload-images.jianshu.io/upload_images/14555448-78bc7ea2d17ae527.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)






![训练2500次，测试集上的正确率是80%](https://upload-images.jianshu.io/upload_images/14555448-7a430c91a090b059.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)



可以看出随着训练次数的增加，训练误差会一直下降，而训练2500次的测试误差要高于训练1500次的测试误差，所以我们在1500次的时候停止会更好。

以上就是机器学习中常用的几种正则化方式，正则化在机器学习中占了很重的地位，在《deep learning》中介绍了更多的正则化方法，有兴趣的同学可以深入研究。

## 参考

[《DeepLearning》 ](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.deeplearningbook.org%2F)
 [吴恩达-深度学习课程](https://links.jianshu.com/go?to=https%3A%2F%2Fmooc.study.163.com%2FsmartSpec%2Fdetail%2F1001319001.htm)

转载
作者：Zero黑羽枫

链接：https://www.jianshu.com/p/569efedf6985

